{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxuw7fISOTNP6k/3NiBLdl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychoi-kr/30days-i18n/blob/main/openai/openai_assistant_with_web_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBTo7UFRQF9w",
        "outputId": "1d5e1f6b-695d-4db9-dbdb-96546838f1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.14.2)\n",
            "Requirement already satisfied: tavily-python in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tavily-python) (2.31.0)\n",
            "Requirement already satisfied: tiktoken==0.5.2 in /usr/local/lib/python3.10/dist-packages (from tavily-python) (0.5.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.2->tavily-python) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tavily-python) (2.0.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import json\n",
        "from tavily import TavilyClient"
      ],
      "metadata": {
        "id": "Nc8JnoW3QLpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "tavily_api_key = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "WKv3-PSUQJzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_instructions = \"\"\"\n",
        "You create a glossary entry in Korean on a given term.\n",
        "\n",
        "Use the web_search tool for initial research to gather and verify information from credible sources. This ensures that definitions are informed by the most recent and reliable data.\n",
        "\n",
        "If the tool does not return any information, abort with fail message.\n",
        "\n",
        "Before including a URL, verify its validity and ensure it leads to the specific content being referenced. Avoid using generic homepage URLs unless they directly relate to the content. Never fabricate a fictional URL.\n",
        "\n",
        "Instead of using honorifics (e.g. \"입니다\") in sentences, use haereahe (e.g. \"이다\") to maintain a direct and concise tone.\n",
        "\n",
        "Follow output format below:\n",
        "```\n",
        "[Term]란 [comprehensive definition in 2-3 paragraphs].\n",
        "\n",
        "### 참고\n",
        "\n",
        "{% for each reference %}\n",
        "- {%=reference in APA style. If the author and site name are not the same, write the author and site name separately.}\n",
        "{% end for %}\n",
        "```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3yv5N7FF92YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "IHv1nhx89Ebx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_client = TavilyClient(api_key=tavily_api_key)\n"
      ],
      "metadata": {
        "id": "ISbAJ_xxBQjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def web_search(query):\n",
        "    search_result = tavily_client.get_search_context(query, search_depth=\"advanced\", max_tokens=8000)\n",
        "    print(search_result)\n",
        "    return search_result"
      ],
      "metadata": {
        "id": "aCqYFlZ4cIuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_json = {\n",
        "    \"name\": \"web_search\",\n",
        "    \"description\": \"Get recent information from the web.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"query\": {\"type\": \"string\", \"description\": \"The search query to use.\"},\n",
        "        },\n",
        "        \"required\": [\"query\"]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "JFgzxJge0of8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = openai_client.beta.assistants.create(\n",
        "    name=\"Define it!\",\n",
        "    instructions=assistant_instructions,\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    tools=[{\"type\": \"function\", \"function\": web_search_json}],\n",
        ")"
      ],
      "metadata": {
        "id": "7p3r_4tW0UwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread = openai_client.beta.threads.create()\n",
        "\n",
        "message = openai_client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Large Multimodal Models\",\n",
        ")"
      ],
      "metadata": {
        "id": "CgEXfc1Y0ld2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = openai_client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        ")"
      ],
      "metadata": {
        "id": "58PIIJN7AXFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    run = openai_client.beta.threads.runs.retrieve(\n",
        "        thread_id=thread.id,\n",
        "        run_id=run.id,\n",
        "    )\n",
        "    run_status = run.status\n",
        "\n",
        "    if run_status == \"requires_action\" and run.required_action is not None:\n",
        "        tools_to_call = run.required_action.submit_tool_outputs.tool_calls\n",
        "        tool_output_array = []\n",
        "        for tool in tools_to_call:\n",
        "            tool_call_id = tool.id\n",
        "            function_name = tool.function.name\n",
        "            function_arg = json.loads(tool.function.arguments)\n",
        "            if function_name == 'web_search':\n",
        "                output = web_search(function_arg[\"query\"])\n",
        "            tool_output_array.append({\"tool_call_id\": tool_call_id, \"output\": output})\n",
        "\n",
        "        run = openai_client.beta.threads.runs.submit_tool_outputs(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id,\n",
        "            tool_outputs=tool_output_array,\n",
        "        )\n",
        "    elif run_status in [\"completed\", \"failed\"]:\n",
        "        break\n",
        "\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac3LmP2V_Rzx",
        "outputId": "6bfc3d4f-5678-41d0-a9bc-d4427d940518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"[\\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.cogitotech.com/blog/large-multimodal-models-the-next-big-gen-ai-wave/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Generative AI blends creativity & technology to delighting humanity\\\\\\\\nDocument Processing Service with Annotation for Data Extraction & Verification\\\\\\\\nComputer Vision Datasets for Object Detection in AI & ML\\\\\\\\nData Annotation and Labeling Consultant for AI, ML\\\\\\\\nContent Moderation Services For Machine Learning\\\\\\\\nNLP Annotation Services for AI-Driven Machine Learning\\\\\\\\nData to Turbocharge AI for Autonomous Vehicles\\\\\\\\nMedical AI Data Solutions\\\\\\\\nAI Training Data for the Logistics Industry\\\\\\\\nAI Data for the Insurance Industry\\\\\\\\nAI Data for Geospatial Applications\\\\\\\\nAI Training Data for Retail\\\\\\\\nAI Training Data for Financial Services\\\\\\\\nAI Data for Robotics Industry\\\\\\\\nAI Data for the E-Commerce Industry\\\\\\\\nAI Training Data for Agritech\\\\\\\\nAI for Security & Surveillance Ecosystem\\\\\\\\nLarge Multimodal Models: The Next Big Gen AI Wave\\\\\\\\nLarge multimodal models involve interpreting a wide range of data for better and intelligent systems. Post navigation\\\\\\\\nPrevious post\\\\\\\\nHow Visual Search is Helpful for Ecommerce Industry?\\\\\\\\nNext post\\\\\\\\nWhat are Chatbots and how they are changing the World of Business?\\\\\\\\n Market Trends in LMMs\\\\\\\\nLeading technology firms and startups are trying their level best to go beyond the AI terrain with the hope of creating new AI models that can work with text as well as images interchangeably.\\\\\\\\n Conclusion\\\\\\\\nThe fine-tuning of foundation models with data that\\\\\\\\u2019s aimed at fulfiling a specific purpose represents a brand new way for democratizing AI and solutions for a bigger and targeted impact. Talk to an Expert\\\\\\\\nInsightful Interpretation of Machine Learning Datasets\\\\\\\\nHow to Improve Accuracy Of Machine Learning Model?\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://towardsdatascience.com/getting-started-with-multimodality-eab5f6453080\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Sign up\\\\\\\\nSign in\\\\\\\\nSign up\\\\\\\\nSign in\\\\\\\\nMember-only story\\\\\\\\nGetting Started with Multimodality\\\\\\\\nUnderstanding vision capabilities of Large Multimodal Models\\\\\\\\nValentina Alto\\\\\\\\nFollow\\\\\\\\nTowards Data Science\\\\\\\\n--\\\\\\\\n3\\\\\\\\nShare\\\\\\\\nThe recent advances in Generative AI have enabled the development of Large Multimodal Models (LMMs) that can process and generate different types of data, such as text, images, audio, and video.\\\\\\\\n The GPT4V (along with its newer version, the GPT-4-turbo vision), has proved extraordinary capabilities, including:\\\\\\\\n--\\\\\\\\n--\\\\\\\\n3\\\\\\\\nWritten by Valentina Alto\\\\\\\\nTowards Data Science\\\\\\\\nData&AI Specialist at @Microsoft | MSc in Data Science | AI, Machine Learning and Running enthusiast\\\\\\\\nHelp\\\\\\\\nStatus\\\\\\\\nAbout\\\\\\\\nCareers\\\\\\\\nBlog\\\\\\\\nPrivacy\\\\\\\\nTerms\\\\\\\\nText to speech\\\\\\\\nTeams GPT-4 can perform various tasks that require both natural language understanding and computer vision, such as image captioning, visual question answering, text-to-image synthesis, and image-to-text translation.\\\\\\\\n One of the most prominent examples of large multimodal models is GPT4V(ision), the latest iteration of the Generative Pre-trained Transformer (GPT) family. However, LMMs are capable of processing data that goes beyond text, including images, audio, and video.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://research.aimultiple.com/large-multimodal-models/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"To stay up-to-date on B2B tech & accelerate your enterprise:\\\\\\\\nNext to Read\\\\\\\\nComparing 10+ LLMOps Tools: A Comprehensive Vendor Benchmark\\\\\\\\nAn In-depth Guide to Meta LLaMa Language Model in 2024\\\\\\\\nLarge Language Model Evaluation in 2024: 5 Methods\\\\\\\\nComments\\\\\\\\nYour email address will not be published. Large Multimodal Models (LMMs) vs Large Language Models (LLMs)\\\\\\\\nLarge multimodal models (LMMs) represent a significant breakthrough, capable of interpreting diverse data types like text, images, and audio. Pre-Training\\\\\\\\n4- Fine-Tuning\\\\\\\\n5- Evaluation and Iteration\\\\\\\\nWhat are some famous large multimodal models?\\\\\\\\n Also, multimodal language model outputs are targeted to be not only textual but visual, auditory etc.\\\\\\\\nMultimodal language models are considered to be next steps toward artificial general intelligence.\\\\\\\\n Although most multimodal large language models today can only use text and image, future research is directed at including audio and video data inputs.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.nocode.ai/what-is-multimodal-ai/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Why are Multimodal AI Models a Big Innovation for Business?\\\\\\\\nGPT-4: A Multimodal AI Model Powerhouse\\\\\\\\nOpenAI's GPT-4 (short for Generative Pre-trained Transformer 4) is a state-of-the-art multimodal AI model that has been making waves in the AI community since it was announced a few days ago. My ambitions grew as we went along\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udc47 pic.twitter.com/oPUzT5Bjzi\\\\\\\\nTo sum up\\\\\\\\nIn a nutshell, multimodal AI models like GPT-4 are reshaping the AI landscape and unlocking new opportunities for businesses across diverse sectors. In this blog post, we'll demystify multimodal AI models, explain their significance in the business world, and delve into the fascinating realm of GPT-4, a leading multimodal AI model by OpenAI.\\\\\\\\n What are Multimodal AI Models?\\\\\\\\nMultimodal AI models are advanced AI systems capable of understanding and generating information from multiple data modalities or sources, such as text, images, audio, and video. Unlike traditional AI models, which are limited to processing only one type of data, multimodal models can analyze and generate insights from various data types, creating a more comprehensive understanding of the input data.\\\\\\\\n\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://cset.georgetown.edu/article/multimodality-tool-use-and-autonomous-agents/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Simple \\\\\\\\u201cscaffolding\\\\\\\\u201d software can interpret the LLM\\\\\\\\u2019s text outputs as commands, and then execute them as appropriate.1\\\\\\\\nWhere an action would require logging into an account (e.g., to make a financial transaction), this can be completed via the human user stepping in, or via the LLM agent having pre-approval (e.g., having access to credentials or operating in a browser where the user is already logged in to relevant accounts). The promise of LLM agents lies in the hope that LLMs can be used to select and carry out actions in environments that are too complex or open-ended to represent formally, without needing large amounts of task-specific data to train a dedicated system \\\\\\\\u201cfrom scratch.\\\\\\\\u201d Researchers have developed open-source tools that map LLM outputs to clicks and keystrokes in a browser, and companies like OpenAI and Microsoft have equipped their models with web browsing capabilities that allow them to access websites and report back to the user about their contents.\\\\\\\\n Thanks to John Bansemer, Matt Burtell, Hanna Dohmen, James Dunham, Rachel Freedman, Krystal Jackson, Ben Murphy, and Vikram Venkatram for their feedback on this post.\\\\\\\\nMarch 8, 2024\\\\\\\\nFoundations\\\\\\\\nRelated Content\\\\\\\\nLarge language models (LLMs), the technology that powers generative artificial intelligence (AI) products like ChatGPT or Google Gemini, are often thought of as chatbots that predict the next word. While the goal of this paper is not to create an exhaustive list of such issues, we will list a few for illustrative purposes:\\\\\\\\nWhile some of these questions, such as those around cybersecurity vulnerabilities, are relevant to deployed systems today, others are mostly relevant to systems still in development.\\\\\\\"}\\\"]\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if run_status == 'completed':\n",
        "    messages = openai_client.beta.threads.messages.list(\n",
        "        thread_id=thread.id,\n",
        "    )\n",
        "    print(messages.data[0].content[0].text.value)\n",
        "else:\n",
        "    print(f\"Run status: {run_status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5fze40QAd31",
        "outputId": "8334d98e-4821-47af-90fd-8d61abc9e467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대형 다중모달 모델(Large Multimodal Models, LMMs)이란 텍스트, 이미지, 오디오 및 비디오와 같은 다양한 유형의 데이터를 처리하고 생성할 수 있는 고급 인공지능 시스템을 의미한다. 이러한 모델들은 최근 생성적 인공지능(Generative AI)의 발전에 의해 가능해졌으며, 특정 데이터 유형에만 한정되지 않고 다양한 데이터 유형을 분석하고 이해할 수 있도록 설계되었다. 따라서 대형 다중모달 모델은 입력 데이터에 대한 더 포괄적인 이해를 생성할 수 있다.\n",
            "\n",
            "GPT4V와 같은 대형 다중모달 모델의 예시에서 볼 수 있듯이, 이러한 모델들은 자연어 이해와 컴퓨터 비전을 필요로 하는 작업들, 예를 들어 이미지 캡션 생성, 시각적 질문 응답, 텍스트-이미지 합성, 이미지-텍스트 번역 등을 수행할 수 있다. 이와 같은 모델들은 텍스트 이외의 데이터도 처리할 수 있는 능력을 갖추고 있어, 시간이 지남에 따라 오디오 및 비디오 데이터의 입력을 포함하는 방향으로 연구가 확장되고 있다고 알려져 있다.\n",
            "\n",
            "### 참고\n",
            "\n",
            "- Cogito. (n.d.). Large Multimodal Models: The Next Big Gen AI Wave. Retrieved from https://www.cogitotech.com/blog/large-multimodal-models-the-next-big-gen-ai-wave/\n",
            "- Alto, V. (n.d.). Getting Started with Multimodality. Towards Data Science. Retrieved from https://towardsdatascience.com/getting-started-with-multimodality-eab5f6453080\n",
            "- AI Multiple. (n.d.). Large Multimodal Models (LMMs) vs Large Language Models (LLMs). Retrieved from https://research.aimultiple.com/large-multimodal-models/\n",
            "- NoCode AI. (n.d.). What are Multimodal AI Models? Retrieved from https://www.nocode.ai/what-is-multimodal-ai/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client.beta.assistants.delete(assistant.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1rDC6ih1HCl",
        "outputId": "ab2819f8-b5a4-4203-ebd4-75570f0783b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AssistantDeleted(id='asst_92ZEe4Z5P53n6C3pO2UTc1Ux', deleted=True, object='assistant.deleted')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}